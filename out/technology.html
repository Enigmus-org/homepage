<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" data-next-head=""/><link rel="icon" type="image/png" href="/favicon-48x48.png" sizes="48x48" data-next-head=""/><link rel="icon" type="image/svg+xml" href="/favicon.svg" data-next-head=""/><link rel="shortcut icon" href="/favicon.ico" data-next-head=""/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" data-next-head=""/><meta name="apple-mobile-web-app-title" content="Enigmus AI" data-next-head=""/><link rel="manifest" href="/site.webmanifest" data-next-head=""/><title data-next-head="">Technology for Private AI on Apple Devices</title><meta name="description" content="Enigmus delivers private, on-device AI for Mac, iPhone, and iPad using Apple&#x27;s MLX framework. By leveraging Apple Silicon&#x27;s unified memory architecture and Metal GPU acceleration, all processing happens locally—data never leaves the device." data-next-head=""/><meta name="author" content="Enigmus" data-next-head=""/><meta name="keywords" content="local ai, private ai, enigmus, local ai for ios, local ai for macos, on-device ai, mlx, apple silicon ai, offline ai" data-next-head=""/><meta property="og:title" content="Technology for Private AI on Apple Devices" data-next-head=""/><meta property="og:description" content="Enigmus delivers private, on-device AI for Mac, iPhone, and iPad using Apple&#x27;s MLX framework. By leveraging Apple Silicon&#x27;s unified memory architecture and Metal GPU acceleration, all processing happens locally—data never leaves the device." data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta property="og:url" content="//technology" data-next-head=""/><meta property="og:site_name" content="Enigmus" data-next-head=""/><meta property="og:locale" content="en_US" data-next-head=""/><meta property="og:image:width" content="1200" data-next-head=""/><meta property="og:image:height" content="630" data-next-head=""/><meta name="twitter:title" content="Technology for Private AI on Apple Devices" data-next-head=""/><meta name="twitter:description" content="Enigmus delivers private, on-device AI for Mac, iPhone, and iPad using Apple&#x27;s MLX framework. By leveraging Apple Silicon&#x27;s unified memory architecture and Metal GPU acceleration, all processing happens locally—data never leaves the device." data-next-head=""/><meta property="og:image" content="//images/ai-technology-review.webp" data-next-head=""/><meta name="twitter:image" content="//images/ai-technology-review.webp" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:site" content="@enigmus_ai" data-next-head=""/><link rel="preload" href="/images/ai-technology-review.webp" as="image" data-next-head=""/><link rel="preload" href="/images/logo-512.png" as="image" data-next-head=""/><link rel="shortcut icon" href="/images/favicon.png"/><link rel="icon" type="image/png" sizes="48x48" href="/favicon-48x48.png"/><link rel="icon" type="image/svg+xml" href="/favicon.svg"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="manifest" href="/site.webmanifest"/><meta name="msapplication-TileColor" content="#1FA3FB"/><meta name="msapplication-TileImage" content="/web-app-manifest-192x192.png"/><meta name="theme-color" media="(prefers-color-scheme: light)" content="#fff"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#000"/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><link data-next-font="" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/_next/static/chunks/f498cc3772add4e2.css" as="style"/><link rel="stylesheet" href="/_next/static/chunks/f498cc3772add4e2.css" data-n-g=""/><link rel="preload" href="/_next/static/chunks/8d8fa24a4981f66a.css" as="style"/><link rel="stylesheet" href="/_next/static/chunks/8d8fa24a4981f66a.css" data-n-p=""/><noscript data-n-css=""></noscript><script src="/_next/static/chunks/1deff985b9e3625e.js" defer=""></script><script src="/_next/static/chunks/3d05902e0a19505d.js" defer=""></script><script src="/_next/static/chunks/96bfc86adc2aa2b0.js" defer=""></script><script src="/_next/static/chunks/turbopack-02833095ea0ce623.js" defer=""></script><script src="/_next/static/chunks/8f65948c416454bf.js" defer=""></script><script src="/_next/static/chunks/0d603c0af6021d88.js" defer=""></script><script src="/_next/static/chunks/6049117865ac50b6.js" defer=""></script><script src="/_next/static/chunks/3977320824122d72.js" defer=""></script><script src="/_next/static/chunks/a6f051acafe7e739.js" defer=""></script><script src="/_next/static/chunks/turbopack-41774b79435e0d27.js" defer=""></script><script src="/_next/static/zmtC6CcDt_NMK1l2nUc--/_ssgManifest.js" defer=""></script><script src="/_next/static/zmtC6CcDt_NMK1l2nUc--/_buildManifest.js" defer=""></script></head><body><div id="__next"><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><header class="header"><nav class="navbar container px-1 sm:px-8"><div class="order-0"><a class="navbar-brand" href="/"><img alt="Enigmus - Private AI, Locally" width="160" height="160" decoding="async" data-nimg="1" class="m-auto" style="color:transparent;height:80px;width:80px" src="/images/logo-512.png"/></a></div><div class="flex items-center space-x-4 xl:space-x-8"><div class="collapse-menu translate-x-full lg:flex lg:translate-x-0"><button class="absolute right-6 top-11 lg:hidden"><svg class="h-4 w-4 fill-current" viewBox="0 0 20 20"><title>Menu Close</title><polygon points="11 9 22 9 22 11 11 11 11 22 9 22 9 11 -2 11 -2 9 9 9 9 -2 11 -2" transform="rotate(45 10 10)"></polygon></svg></button><ul id="nav-menu" class="navbar-nav w-full md:w-auto md:space-x-1 lg:flex xl:space-x-2"><li class="nav-item"><a class="nav-link block false" href="/">Home</a></li><li class="nav-item"><a class="nav-link block false" href="/download">Download</a></li><li class="nav-item"><a class="nav-link block false" href="/ai-and-privacy">AI and Privacy</a></li><li class="nav-item"><a class="nav-link block active" href="/technology">Technology</a></li><li class="nav-item nav-dropdown group relative"><span class="nav-link false inline-flex items-center">Pages<svg class="h-4 w-4 fill-current" viewBox="0 0 20 20"><path d="M9.293 12.95l.707.707L15.657 8l-1.414-1.414L10 10.828 5.757 6.586 4.343 8z"></path></svg></span><ul class="nav-dropdown-list hidden transition-all duration-300 group-hover:top-[46px] group-hover:block md:invisible md:absolute md:top-[60px] md:block md:opacity-0 md:group-hover:visible md:group-hover:opacity-100"><li class="nav-dropdown-item"><a class="nav-dropdown-link block false" href="/categories">Categories</a></li></ul></li></ul><ul class="socials"><li class="inline-block"><a aria-label="twitter" href="https://twitter.com/" target="_blank" rel="noopener noreferrer nofollow"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M496 109.5a201.8 201.8 0 01-56.55 15.3 97.51 97.51 0 0043.33-53.6 197.74 197.74 0 01-62.56 23.5A99.14 99.14 0 00348.31 64c-54.42 0-98.46 43.4-98.46 96.9a93.21 93.21 0 002.54 22.1 280.7 280.7 0 01-203-101.3A95.69 95.69 0 0036 130.4c0 33.6 17.53 63.3 44 80.7A97.5 97.5 0 0135.22 199v1.2c0 47 34 86.1 79 95a100.76 100.76 0 01-25.94 3.4 94.38 94.38 0 01-18.51-1.8c12.51 38.5 48.92 66.5 92.05 67.3A199.59 199.59 0 0139.5 405.6a203 203 0 01-23.5-1.4A278.68 278.68 0 00166.74 448c181.36 0 280.44-147.7 280.44-275.8 0-4.2-.11-8.4-.31-12.5A198.48 198.48 0 00496 109.5z"></path></svg></a></li><li class="inline-block"><a aria-label="github" href="https://github.com/Enigmus-org/enigmus-desktop" target="_blank" rel="noopener noreferrer nofollow"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M256 32C132.3 32 32 134.9 32 261.7c0 101.5 64.2 187.5 153.2 217.9a17.56 17.56 0 003.8.4c8.3 0 11.5-6.1 11.5-11.4 0-5.5-.2-19.9-.3-39.1a102.4 102.4 0 01-22.6 2.7c-43.1 0-52.9-33.5-52.9-33.5-10.2-26.5-24.9-33.6-24.9-33.6-19.5-13.7-.1-14.1 1.4-14.1h.1c22.5 2 34.3 23.8 34.3 23.8 11.2 19.6 26.2 25.1 39.6 25.1a63 63 0 0025.6-6c2-14.8 7.8-24.9 14.2-30.7-49.7-5.8-102-25.5-102-113.5 0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8a18.64 18.64 0 015-.5c8.1 0 26.4 3.1 56.6 24.1a208.21 208.21 0 01112.2 0c30.2-21 48.5-24.1 56.6-24.1a18.64 18.64 0 015 .5c12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6 0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5 0 30.7-.3 55.5-.3 63 0 5.4 3.1 11.5 11.4 11.5a19.35 19.35 0 004-.4C415.9 449.2 480 363.1 480 261.7 480 134.9 379.7 32 256 32z"></path></svg></a></li></ul></div><button aria-label="Toggle Theme" type="button" class="ml-1 mr-1 h-8 w-8 rounded p-1 sm:ml-4"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="text-gray-900 dark:text-gray-100"><path d="M17.293 13.293A8 8 0 016.707 2.707a8.001 8.001 0 1010.586 10.586z"></path></svg></button><div class="search-icon"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M456.69 421.39L362.6 327.3a173.81 173.81 0 0034.84-104.58C397.44 126.38 319.06 48 222.72 48S48 126.38 48 222.72s78.38 174.72 174.72 174.72A173.81 173.81 0 00327.3 362.6l94.09 94.09a25 25 0 0035.3-35.3zM97.92 222.72a124.8 124.8 0 11124.8 124.8 124.95 124.95 0 01-124.8-124.8z"></path></svg></div><button class="inline-flex h-10 w-10 items-center justify-center rounded-full bg-primary text-white lg:hidden"><svg class="h-4 w-4 fill-current" viewBox="0 0 20 20"><title>Menu Open</title><path d="M0 3h20v2H0V3z m0 6h20v2H0V9z m0 6h20v2H0V0z"></path></svg></button></div><div class="search-modal "><button class="search-close"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path fill="none" stroke-miterlimit="10" stroke-width="32" d="M448 256c0-106-86-192-192-192S64 150 64 256s86 192 192 192 192-86 192-192z"></path><path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-width="32" d="M320 320L192 192m0 128l128-128"></path></svg></button><input type="text" class="form-input bg-body placeholder:text-base dark:bg-darkmode-body" id="searchModal" placeholder="Type and hit enter..."/></div></nav></header><main><section class="section "><div class="container text-center"><h1 class="sr-only">Technology for Private AI on Apple Devices</h1><div class="mb-8"><img alt="Technology for Private AI on Apple Devices" width="1298" height="616" decoding="async" data-nimg="1" class="rounded-lg" style="color:transparent" src="/images/ai-technology-review.webp"/></div><div class="content text-left mx-auto"><h1 id="private-ai-powered-by-apple-silicon">Private AI, Powered by Apple Silicon</h1>
<p>Enigmus is built exclusively for Apple platforms, leveraging <strong>MLX</strong>—Apple&#x27;s machine learning framework—to deliver private AI on Mac, iPhone, and iPad. By running entirely on-device, data never leaves the hardware.</p>
<h2 id="why-apple-silicon">Why Apple Silicon?</h2>
<p>Apple&#x27;s M-series chips (M1, M2, M3, M4, and M5) revolutionized what&#x27;s possible for on-device AI. The key innovation is <strong>unified memory architecture</strong>—CPU, GPU, and Neural Engine all share the same memory pool, eliminating the data transfer bottlenecks that plague traditional systems.</p>
<p>This means large language models can run efficiently without expensive dedicated GPUs. A MacBook, iMac, or iPhone becomes a capable AI workstation.</p>
<h2 id="mlx-apples-ml-framework">MLX: Apple&#x27;s ML Framework</h2>
<p>MLX is Apple&#x27;s array framework for machine learning, purpose-built for Apple Silicon. At WWDC 2025, Apple signaled MLX as a strategic component of their AI ecosystem, with deep integration into macOS and iOS.</p>
<h3 id="key-advantages">Key Advantages</h3>
<ul>
<li><strong>Unified Memory</strong>: Arrays live in shared memory—operations run on CPU, GPU, or Neural Engine without data copying</li>
<li><strong>Metal GPU Acceleration</strong>: Purpose-built for Apple&#x27;s Metal framework, maximizing performance on Apple hardware</li>
<li><strong>Native Swift Support</strong>: First-class Swift API makes it perfect for iOS and macOS app development</li>
<li><strong>Neural Engine Integration</strong>: On M5 chips, MLX leverages dedicated Neural Accelerators for matrix operations</li>
</ul>
<h3 id="on-device-benefits">On-Device Benefits</h3>
<p>Running AI locally on Apple devices provides:</p>
<ul>
<li><strong>Complete Privacy</strong>: Conversations and data never leave the device</li>
<li><strong>No API Costs</strong>: No per-token fees or subscription requirements</li>
<li><strong>Offline Capable</strong>: Works without internet once the model is downloaded</li>
<li><strong>Low Latency</strong>: Instant responses without network round-trips</li>
</ul>
<h2 id="supported-models">Supported Models</h2>
<p>Enigmus supports leading models optimized for Apple Silicon:</p>
<h3 id="gpt-oss-by-openai">GPT-OSS by OpenAI</h3>
<p>OpenAI&#x27;s first open-weight models since GPT-2, released August 2025:</p>
<ul>
<li><strong>gpt-oss-20b</strong>: 21B parameters, runs within 16GB memory—ideal for M1/M2/M3 Macs</li>
<li><strong>gpt-oss-120b</strong>: 117B parameters for high-memory configurations</li>
</ul>
<p>Both use mixture-of-experts (MoE) architecture with 4-bit quantization, delivering excellent performance on Apple Silicon.</p>
<h3 id="qwen3-by-alibaba">Qwen3 by Alibaba</h3>
<p>Alibaba&#x27;s hybrid reasoning models, released April 2025:</p>
<ul>
<li><strong>Qwen3-0.6B / Qwen3-1.7B / Qwen3-4B / Qwen3-8B</strong>: Models for iPhone and iPad, with Qwen3-8B for high-memory devices</li>
<li><strong>Qwen3-14B / Qwen3-32B</strong>: Full-featured models for Mac</li>
<li><strong>Qwen3-30B-A3B</strong>: Sparse MoE variant—32B-class performance with only 3B parameters active</li>
<li><strong>Qwen3-Next-80B / Qwen3-235B-A22B</strong>: Large models for high-memory Macs (64GB+)</li>
</ul>
<p>Qwen3 features hybrid reasoning (toggle between fast and deep thinking), 128K context window, and support for 119 languages.</p>
<h2 id="performance-on-apple-devices">Performance on Apple Devices</h2>
<h3 id="mac-apple-silicon">Mac (Apple Silicon)</h3>
<p>On M1 and newer Macs, Enigmus delivers responsive AI interactions:</p>
<ul>
<li><strong>M1/M2 (8GB)</strong>: Qwen3-0.6B and Qwen3-1.7B run smoothly for everyday tasks</li>
<li><strong>M1/M2 Pro (16GB+)</strong>: GPT-OSS-20b and Qwen3-14B for advanced use cases</li>
<li><strong>M3/M4 Max (64GB+)</strong>: Run large models including Qwen3-32B, Qwen3-Next-80B</li>
<li><strong>M3/M4 Max (128GB+)</strong>: Run the largest models including Qwen3-235B-A22B</li>
<li><strong>M5 with Neural Accelerators</strong>: Optimized matrix operations for fastest inference</li>
</ul>
<h3 id="iphone--ipad-ios-18">iPhone &amp; iPad (iOS 18+)</h3>
<p>Enigmus brings on-device AI to mobile:</p>
<ul>
<li><strong>iPhone 13+ / iPad</strong>: Run Qwen3-0.6B, Qwen3-1.7B, Qwen3-4B, or Qwen3-8B (high-memory devices)</li>
<li><strong>Increased Memory Entitlement</strong>: Enables larger models on capable devices</li>
<li><strong>Metal GPU Required</strong>: Real device needed (simulators not supported)</li>
</ul>
<h2 id="the-privacy-advantage">The Privacy Advantage</h2>
<p>Unlike cloud-based AI services, Enigmus processes everything locally. When asking a question, drafting an email, or analyzing a document:</p>
<ol>
<li>Input stays on the device</li>
<li>The AI model runs on Apple Silicon</li>
<li>The response is generated locally</li>
<li>Nothing is uploaded to external servers</li>
</ol>
<p>This architecture ensures privacy by design—data remains on the device at all times.</p>
<hr/>
<h2 id="frequently-asked-questions">Frequently Asked Questions</h2>
<details><summary><strong>What is MLX and why does Enigmus use it?</strong></summary><p>MLX is Apple&#x27;s open-source machine learning framework, designed specifically for Apple Silicon. Unlike other ML frameworks, MLX uses a <strong>unified memory model</strong> where arrays live in shared memory—allowing operations to run on CPU, GPU, or Neural Engine without copying data between them. This makes it exceptionally efficient for running large language models on Mac, iPhone, and iPad.</p><p>At WWDC 2025, Apple announced deeper MLX integration into macOS and iOS, signaling it as a core component of their AI strategy.</p><p><em>Sources: <a href="https://opensource.apple.com/projects/mlx/">Apple MLX Open Source</a> · <a href="https://github.com/ml-explore/mlx">GitHub - ml-explore/mlx</a> · <a href="https://developer.apple.com/videos/play/wwdc2025/298/">WWDC 2025 MLX Session</a></em></p></details>
<details><summary><strong>What&#x27;s the minimum Mac configuration to run Enigmus?</strong></summary><p>Enigmus requires <strong>any Mac with Apple Silicon</strong> (M1 or newer) running <strong>macOS 14 Sonoma</strong> or later. The experience scales with your hardware:</p><ul>
<li><strong>8GB RAM</strong>: Run compact models like Qwen3-0.6B or Qwen3-1.7B for everyday tasks</li>
<li><strong>16GB RAM</strong>: Run mid-size models like GPT-OSS-20b or Qwen3-14B</li>
<li><strong>32GB+ RAM</strong>: Run larger models with better context handling</li>
<li><strong>64GB+ RAM</strong>: Run large models like Qwen3-32B, Qwen3-Next-80B</li>
<li><strong>128GB+ RAM</strong>: Run the largest models like Qwen3-235B-A22B</li>
</ul><p>The M5 chips with Neural Accelerators provide the fastest inference thanks to dedicated matrix multiplication hardware.</p><p><em>Sources: <a href="https://ml-explore.github.io/mlx/build/html/index.html">MLX Documentation</a> · <a href="https://machinelearning.apple.com/research/exploring-llms-mlx-m5">Apple M5 Neural Accelerators</a></em></p></details>
<details><summary><strong>Can I run Enigmus on iPhone or iPad?</strong></summary><p>Yes. Enigmus supports <strong>iOS 18+</strong> on devices with sufficient hardware:</p><ul>
<li><strong>iPhone 13</strong> or newer (A15 chip or later)</li>
<li><strong>iPad</strong> with A15 chip or later</li>
</ul><p>A real device is required—iOS Simulators don&#x27;t support the Metal GPU features MLX requires. For larger models, the &quot;Increased Memory Limit&quot; entitlement must be enabled in device settings.</p><p>The Qwen3-0.6B and Qwen3-1.7B models run well on all supported devices, with Qwen3-4B and Qwen3-8B available on high-memory devices.</p><p><em>Sources: <a href="https://medium.com/@cetinibrahim/mlx-swift-run-llms-in-ios-apps-8f89c1123588">MLX Swift on iOS</a> · <a href="https://github.com/ml-explore/mlx-swift">GitHub - ml-explore/mlx-swift</a></em></p></details>
<details><summary><strong>What is GPT-OSS and how does it compare to ChatGPT?</strong></summary><p>GPT-OSS is OpenAI&#x27;s first <strong>open-weight model family</strong> since GPT-2, released in August 2025. It includes two variants:</p><ul>
<li><strong>gpt-oss-20b</strong>: 21 billion parameters, fits in 16GB memory</li>
<li><strong>gpt-oss-120b</strong>: 117 billion parameters for high-end systems</li>
</ul><p>Both use a mixture-of-experts (MoE) architecture with 4-bit quantization (MXFP4). The gpt-oss-120b matches or exceeds OpenAI&#x27;s o4-mini on benchmarks for coding, math, and tool use—running entirely on-device with no API costs or data sharing.</p><p><em>Sources: <a href="https://openai.com/index/introducing-gpt-oss/">Introducing GPT-OSS | OpenAI</a> · <a href="https://openai.com/index/gpt-oss-model-card/">GPT-OSS Model Card</a> · <a href="https://github.com/openai/gpt-oss">GitHub - openai/gpt-oss</a></em></p></details>
<details><summary><strong>What makes Qwen3 special for local AI?</strong></summary><p>Qwen3, released by Alibaba in April 2025, offers several advantages for local deployment:</p><ul>
<li><strong>Hybrid reasoning</strong>: Toggle between fast responses and deep thinking mode</li>
<li><strong>Efficient variants</strong>: The Qwen3-30B-A3B uses only 3B active parameters while delivering 32B-class performance</li>
<li><strong>Massive context</strong>: 128K token context window (1M tokens in Qwen3-2507)</li>
<li><strong>Multilingual</strong>: Supports 119 languages and dialects</li>
<li><strong>Size range</strong>: From 0.6B (ultra-light) to 32B (full-featured)</li>
</ul><p>The compact Qwen3-0.6B and Qwen3-1.7B models work on all supported iOS devices, Qwen3-4B and Qwen3-8B on high-memory devices, while larger variants shine on Mac.</p><p><em>Sources: <a href="https://techcrunch.com/2025/04/28/alibaba-unveils-qwen-3-a-family-of-hybrid-ai-reasoning-models/">Alibaba Qwen3 Announcement</a> · <a href="https://github.com/QwenLM/Qwen3">GitHub - QwenLM/Qwen3</a> · <a href="https://huggingface.co/Qwen">Qwen on Hugging Face</a></em></p></details>
<details><summary><strong>Is my data really private with Enigmus?</strong></summary><p><strong>Yes, completely.</strong> Enigmus processes everything on-device using Apple&#x27;s MLX framework. Here&#x27;s what that means:</p><ol>
<li><strong>No cloud connection required</strong>: Once a model is downloaded, Enigmus works entirely offline</li>
<li><strong>No data transmission</strong>: Prompts, documents, and conversations never leave the Mac, iPhone, or iPad</li>
<li><strong>No telemetry</strong>: No usage data, analytics, or interaction information is collected</li>
<li><strong>Local ownership</strong>: Everything stays in local storage under user control</li>
</ol><p>This is fundamentally different from cloud AI services like ChatGPT or Claude, which process data on remote servers. With Enigmus, privacy isn&#x27;t a policy—it&#x27;s architecture.</p><p><em>Sources: <a href="https://ml-explore.github.io/mlx/build/html/unified_memory.html">MLX Unified Memory Model</a> · <a href="https://www.swift.org/blog/mlx-swift/">On-device ML with MLX Swift</a></em></p></details></div></div></section></main><footer class="section relative mt-12 pt-[70px] pb-[50px]"><img alt="footer background" loading="lazy" decoding="async" data-nimg="fill" class="-z-[1] object-cover object-left  md:object-top" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/images/footer-bg-shape.svg"/><div class="container text-center"><div class="mb-6 inline-flex"><a class="navbar-brand" href="/"><img alt="Enigmus - Private AI, Locally" width="160" height="160" decoding="async" data-nimg="1" class="m-auto" style="color:transparent;height:80px;width:80px" src="/images/logo-512.png"/></a></div><p class="max-w-[638px] mx-auto"><span class="font-bold text-primary">Enigmus</span> - private AI for Mac, iPhone, and iPad. Powered by Apple&#39;s MLX framework, all processing happens locally on-device—data never leaves the hardware.</p><ul class="mb-12 mt-6 flex-wrap space-x-2 lg:space-x-4"><li class="inline-block"><a class="p-2 font-bold text-dark hover:text-primary dark:text-darkmode-light lg:p-4" href="/">Home</a></li><li class="inline-block"><a class="p-2 font-bold text-dark hover:text-primary dark:text-darkmode-light lg:p-4" href="/download">Download</a></li><li class="inline-block"><a class="p-2 font-bold text-dark hover:text-primary dark:text-darkmode-light lg:p-4" href="/ai-and-privacy">AI and Privacy</a></li><li class="inline-block"><a class="p-2 font-bold text-dark hover:text-primary dark:text-darkmode-light lg:p-4" href="/technology">Technology</a></li><li class="inline-block"><a class="p-2 font-bold text-dark hover:text-primary dark:text-darkmode-light lg:p-4" href="/privacy-policy">Privacy Policy</a></li><li class="inline-block"><a class="p-2 font-bold text-dark hover:text-primary dark:text-darkmode-light lg:p-4" href="/terms-of-usage">Terms of Service</a></li></ul><div class="inline-flex"><ul class="socials mb-12 justify-center"><li class="inline-block"><a aria-label="twitter" href="https://twitter.com/" target="_blank" rel="noopener noreferrer nofollow"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M496 109.5a201.8 201.8 0 01-56.55 15.3 97.51 97.51 0 0043.33-53.6 197.74 197.74 0 01-62.56 23.5A99.14 99.14 0 00348.31 64c-54.42 0-98.46 43.4-98.46 96.9a93.21 93.21 0 002.54 22.1 280.7 280.7 0 01-203-101.3A95.69 95.69 0 0036 130.4c0 33.6 17.53 63.3 44 80.7A97.5 97.5 0 0135.22 199v1.2c0 47 34 86.1 79 95a100.76 100.76 0 01-25.94 3.4 94.38 94.38 0 01-18.51-1.8c12.51 38.5 48.92 66.5 92.05 67.3A199.59 199.59 0 0139.5 405.6a203 203 0 01-23.5-1.4A278.68 278.68 0 00166.74 448c181.36 0 280.44-147.7 280.44-275.8 0-4.2-.11-8.4-.31-12.5A198.48 198.48 0 00496 109.5z"></path></svg></a></li><li class="inline-block"><a aria-label="github" href="https://github.com/Enigmus-org/enigmus-desktop" target="_blank" rel="noopener noreferrer nofollow"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M256 32C132.3 32 32 134.9 32 261.7c0 101.5 64.2 187.5 153.2 217.9a17.56 17.56 0 003.8.4c8.3 0 11.5-6.1 11.5-11.4 0-5.5-.2-19.9-.3-39.1a102.4 102.4 0 01-22.6 2.7c-43.1 0-52.9-33.5-52.9-33.5-10.2-26.5-24.9-33.6-24.9-33.6-19.5-13.7-.1-14.1 1.4-14.1h.1c22.5 2 34.3 23.8 34.3 23.8 11.2 19.6 26.2 25.1 39.6 25.1a63 63 0 0025.6-6c2-14.8 7.8-24.9 14.2-30.7-49.7-5.8-102-25.5-102-113.5 0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8a18.64 18.64 0 015-.5c8.1 0 26.4 3.1 56.6 24.1a208.21 208.21 0 01112.2 0c30.2-21 48.5-24.1 56.6-24.1a18.64 18.64 0 015 .5c12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6 0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5 0 30.7-.3 55.5-.3 63 0 5.4 3.1 11.5 11.4 11.5a19.35 19.35 0 004-.4C415.9 449.2 480 363.1 480 261.7 480 134.9 379.7 32 256 32z"></path></svg></a></li></ul></div><p>Designed and Developed By <a href="https://Enigmus.cc/">Enigmus.cc</a></p></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"slug":"technology","data":{"frontmatter":{"title":"Technology for Private AI on Apple Devices","image":"/images/ai-technology-review.webp","description":"Enigmus delivers private, on-device AI for Mac, iPhone, and iPad using Apple's MLX framework. By leveraging Apple Silicon's unified memory architecture and Metal GPU acceleration, all processing happens locally—data never leaves the device.","layout":"text2image"},"content":"\n# Private AI, Powered by Apple Silicon\n\nEnigmus is built exclusively for Apple platforms, leveraging **MLX**—Apple's machine learning framework—to deliver private AI on Mac, iPhone, and iPad. By running entirely on-device, data never leaves the hardware.\n\n## Why Apple Silicon?\n\nApple's M-series chips (M1, M2, M3, M4, and M5) revolutionized what's possible for on-device AI. The key innovation is **unified memory architecture**—CPU, GPU, and Neural Engine all share the same memory pool, eliminating the data transfer bottlenecks that plague traditional systems.\n\nThis means large language models can run efficiently without expensive dedicated GPUs. A MacBook, iMac, or iPhone becomes a capable AI workstation.\n\n## MLX: Apple's ML Framework\n\nMLX is Apple's array framework for machine learning, purpose-built for Apple Silicon. At WWDC 2025, Apple signaled MLX as a strategic component of their AI ecosystem, with deep integration into macOS and iOS.\n\n### Key Advantages\n\n- **Unified Memory**: Arrays live in shared memory—operations run on CPU, GPU, or Neural Engine without data copying\n- **Metal GPU Acceleration**: Purpose-built for Apple's Metal framework, maximizing performance on Apple hardware\n- **Native Swift Support**: First-class Swift API makes it perfect for iOS and macOS app development\n- **Neural Engine Integration**: On M5 chips, MLX leverages dedicated Neural Accelerators for matrix operations\n\n### On-Device Benefits\n\nRunning AI locally on Apple devices provides:\n\n- **Complete Privacy**: Conversations and data never leave the device\n- **No API Costs**: No per-token fees or subscription requirements\n- **Offline Capable**: Works without internet once the model is downloaded\n- **Low Latency**: Instant responses without network round-trips\n\n## Supported Models\n\nEnigmus supports leading models optimized for Apple Silicon:\n\n### GPT-OSS by OpenAI\n\nOpenAI's first open-weight models since GPT-2, released August 2025:\n\n- **gpt-oss-20b**: 21B parameters, runs within 16GB memory—ideal for M1/M2/M3 Macs\n- **gpt-oss-120b**: 117B parameters for high-memory configurations\n\nBoth use mixture-of-experts (MoE) architecture with 4-bit quantization, delivering excellent performance on Apple Silicon.\n\n### Qwen3 by Alibaba\n\nAlibaba's hybrid reasoning models, released April 2025:\n\n- **Qwen3-0.6B / Qwen3-1.7B / Qwen3-4B / Qwen3-8B**: Models for iPhone and iPad, with Qwen3-8B for high-memory devices\n- **Qwen3-14B / Qwen3-32B**: Full-featured models for Mac\n- **Qwen3-30B-A3B**: Sparse MoE variant—32B-class performance with only 3B parameters active\n- **Qwen3-Next-80B / Qwen3-235B-A22B**: Large models for high-memory Macs (64GB+)\n\nQwen3 features hybrid reasoning (toggle between fast and deep thinking), 128K context window, and support for 119 languages.\n\n## Performance on Apple Devices\n\n### Mac (Apple Silicon)\n\nOn M1 and newer Macs, Enigmus delivers responsive AI interactions:\n\n- **M1/M2 (8GB)**: Qwen3-0.6B and Qwen3-1.7B run smoothly for everyday tasks\n- **M1/M2 Pro (16GB+)**: GPT-OSS-20b and Qwen3-14B for advanced use cases\n- **M3/M4 Max (64GB+)**: Run large models including Qwen3-32B, Qwen3-Next-80B\n- **M3/M4 Max (128GB+)**: Run the largest models including Qwen3-235B-A22B\n- **M5 with Neural Accelerators**: Optimized matrix operations for fastest inference\n\n### iPhone \u0026 iPad (iOS 18+)\n\nEnigmus brings on-device AI to mobile:\n\n- **iPhone 13+ / iPad**: Run Qwen3-0.6B, Qwen3-1.7B, Qwen3-4B, or Qwen3-8B (high-memory devices)\n- **Increased Memory Entitlement**: Enables larger models on capable devices\n- **Metal GPU Required**: Real device needed (simulators not supported)\n\n## The Privacy Advantage\n\nUnlike cloud-based AI services, Enigmus processes everything locally. When asking a question, drafting an email, or analyzing a document:\n\n1. Input stays on the device\n2. The AI model runs on Apple Silicon\n3. The response is generated locally\n4. Nothing is uploaded to external servers\n\nThis architecture ensures privacy by design—data remains on the device at all times.\n\n---\n\n## Frequently Asked Questions\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cstrong\u003eWhat is MLX and why does Enigmus use it?\u003c/strong\u003e\u003c/summary\u003e\n\nMLX is Apple's open-source machine learning framework, designed specifically for Apple Silicon. Unlike other ML frameworks, MLX uses a **unified memory model** where arrays live in shared memory—allowing operations to run on CPU, GPU, or Neural Engine without copying data between them. This makes it exceptionally efficient for running large language models on Mac, iPhone, and iPad.\n\nAt WWDC 2025, Apple announced deeper MLX integration into macOS and iOS, signaling it as a core component of their AI strategy.\n\n*Sources: [Apple MLX Open Source](https://opensource.apple.com/projects/mlx/) · [GitHub - ml-explore/mlx](https://github.com/ml-explore/mlx) · [WWDC 2025 MLX Session](https://developer.apple.com/videos/play/wwdc2025/298/)*\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cstrong\u003eWhat's the minimum Mac configuration to run Enigmus?\u003c/strong\u003e\u003c/summary\u003e\n\nEnigmus requires **any Mac with Apple Silicon** (M1 or newer) running **macOS 14 Sonoma** or later. The experience scales with your hardware:\n\n- **8GB RAM**: Run compact models like Qwen3-0.6B or Qwen3-1.7B for everyday tasks\n- **16GB RAM**: Run mid-size models like GPT-OSS-20b or Qwen3-14B\n- **32GB+ RAM**: Run larger models with better context handling\n- **64GB+ RAM**: Run large models like Qwen3-32B, Qwen3-Next-80B\n- **128GB+ RAM**: Run the largest models like Qwen3-235B-A22B\n\nThe M5 chips with Neural Accelerators provide the fastest inference thanks to dedicated matrix multiplication hardware.\n\n*Sources: [MLX Documentation](https://ml-explore.github.io/mlx/build/html/index.html) · [Apple M5 Neural Accelerators](https://machinelearning.apple.com/research/exploring-llms-mlx-m5)*\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cstrong\u003eCan I run Enigmus on iPhone or iPad?\u003c/strong\u003e\u003c/summary\u003e\n\nYes. Enigmus supports **iOS 18+** on devices with sufficient hardware:\n\n- **iPhone 13** or newer (A15 chip or later)\n- **iPad** with A15 chip or later\n\nA real device is required—iOS Simulators don't support the Metal GPU features MLX requires. For larger models, the \"Increased Memory Limit\" entitlement must be enabled in device settings.\n\nThe Qwen3-0.6B and Qwen3-1.7B models run well on all supported devices, with Qwen3-4B and Qwen3-8B available on high-memory devices.\n\n*Sources: [MLX Swift on iOS](https://medium.com/@cetinibrahim/mlx-swift-run-llms-in-ios-apps-8f89c1123588) · [GitHub - ml-explore/mlx-swift](https://github.com/ml-explore/mlx-swift)*\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cstrong\u003eWhat is GPT-OSS and how does it compare to ChatGPT?\u003c/strong\u003e\u003c/summary\u003e\n\nGPT-OSS is OpenAI's first **open-weight model family** since GPT-2, released in August 2025. It includes two variants:\n\n- **gpt-oss-20b**: 21 billion parameters, fits in 16GB memory\n- **gpt-oss-120b**: 117 billion parameters for high-end systems\n\nBoth use a mixture-of-experts (MoE) architecture with 4-bit quantization (MXFP4). The gpt-oss-120b matches or exceeds OpenAI's o4-mini on benchmarks for coding, math, and tool use—running entirely on-device with no API costs or data sharing.\n\n*Sources: [Introducing GPT-OSS | OpenAI](https://openai.com/index/introducing-gpt-oss/) · [GPT-OSS Model Card](https://openai.com/index/gpt-oss-model-card/) · [GitHub - openai/gpt-oss](https://github.com/openai/gpt-oss)*\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cstrong\u003eWhat makes Qwen3 special for local AI?\u003c/strong\u003e\u003c/summary\u003e\n\nQwen3, released by Alibaba in April 2025, offers several advantages for local deployment:\n\n- **Hybrid reasoning**: Toggle between fast responses and deep thinking mode\n- **Efficient variants**: The Qwen3-30B-A3B uses only 3B active parameters while delivering 32B-class performance\n- **Massive context**: 128K token context window (1M tokens in Qwen3-2507)\n- **Multilingual**: Supports 119 languages and dialects\n- **Size range**: From 0.6B (ultra-light) to 32B (full-featured)\n\nThe compact Qwen3-0.6B and Qwen3-1.7B models work on all supported iOS devices, Qwen3-4B and Qwen3-8B on high-memory devices, while larger variants shine on Mac.\n\n*Sources: [Alibaba Qwen3 Announcement](https://techcrunch.com/2025/04/28/alibaba-unveils-qwen-3-a-family-of-hybrid-ai-reasoning-models/) · [GitHub - QwenLM/Qwen3](https://github.com/QwenLM/Qwen3) · [Qwen on Hugging Face](https://huggingface.co/Qwen)*\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cstrong\u003eIs my data really private with Enigmus?\u003c/strong\u003e\u003c/summary\u003e\n\n**Yes, completely.** Enigmus processes everything on-device using Apple's MLX framework. Here's what that means:\n\n1. **No cloud connection required**: Once a model is downloaded, Enigmus works entirely offline\n2. **No data transmission**: Prompts, documents, and conversations never leave the Mac, iPhone, or iPad\n3. **No telemetry**: No usage data, analytics, or interaction information is collected\n4. **Local ownership**: Everything stays in local storage under user control\n\nThis is fundamentally different from cloud AI services like ChatGPT or Claude, which process data on remote servers. With Enigmus, privacy isn't a policy—it's architecture.\n\n*Sources: [MLX Unified Memory Model](https://ml-explore.github.io/mlx/build/html/unified_memory.html) · [On-device ML with MLX Swift](https://www.swift.org/blog/mlx-swift/)*\n\n\u003c/details\u003e\n","mdxContent":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    h1: \"h1\",\n    p: \"p\",\n    strong: \"strong\",\n    h2: \"h2\",\n    h3: \"h3\",\n    ul: \"ul\",\n    li: \"li\",\n    ol: \"ol\",\n    hr: \"hr\",\n    em: \"em\",\n    a: \"a\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.h1, {\n      id: \"private-ai-powered-by-apple-silicon\",\n      children: \"Private AI, Powered by Apple Silicon\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Enigmus is built exclusively for Apple platforms, leveraging \", _jsx(_components.strong, {\n        children: \"MLX\"\n      }), \"—Apple's machine learning framework—to deliver private AI on Mac, iPhone, and iPad. By running entirely on-device, data never leaves the hardware.\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"why-apple-silicon\",\n      children: \"Why Apple Silicon?\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Apple's M-series chips (M1, M2, M3, M4, and M5) revolutionized what's possible for on-device AI. The key innovation is \", _jsx(_components.strong, {\n        children: \"unified memory architecture\"\n      }), \"—CPU, GPU, and Neural Engine all share the same memory pool, eliminating the data transfer bottlenecks that plague traditional systems.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This means large language models can run efficiently without expensive dedicated GPUs. A MacBook, iMac, or iPhone becomes a capable AI workstation.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"mlx-apples-ml-framework\",\n      children: \"MLX: Apple's ML Framework\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"MLX is Apple's array framework for machine learning, purpose-built for Apple Silicon. At WWDC 2025, Apple signaled MLX as a strategic component of their AI ecosystem, with deep integration into macOS and iOS.\"\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"key-advantages\",\n      children: \"Key Advantages\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Unified Memory\"\n        }), \": Arrays live in shared memory—operations run on CPU, GPU, or Neural Engine without data copying\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Metal GPU Acceleration\"\n        }), \": Purpose-built for Apple's Metal framework, maximizing performance on Apple hardware\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Native Swift Support\"\n        }), \": First-class Swift API makes it perfect for iOS and macOS app development\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Neural Engine Integration\"\n        }), \": On M5 chips, MLX leverages dedicated Neural Accelerators for matrix operations\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"on-device-benefits\",\n      children: \"On-Device Benefits\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Running AI locally on Apple devices provides:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Complete Privacy\"\n        }), \": Conversations and data never leave the device\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"No API Costs\"\n        }), \": No per-token fees or subscription requirements\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Offline Capable\"\n        }), \": Works without internet once the model is downloaded\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Low Latency\"\n        }), \": Instant responses without network round-trips\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"supported-models\",\n      children: \"Supported Models\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Enigmus supports leading models optimized for Apple Silicon:\"\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"gpt-oss-by-openai\",\n      children: \"GPT-OSS by OpenAI\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"OpenAI's first open-weight models since GPT-2, released August 2025:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"gpt-oss-20b\"\n        }), \": 21B parameters, runs within 16GB memory—ideal for M1/M2/M3 Macs\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"gpt-oss-120b\"\n        }), \": 117B parameters for high-memory configurations\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Both use mixture-of-experts (MoE) architecture with 4-bit quantization, delivering excellent performance on Apple Silicon.\"\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"qwen3-by-alibaba\",\n      children: \"Qwen3 by Alibaba\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Alibaba's hybrid reasoning models, released April 2025:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Qwen3-0.6B / Qwen3-1.7B / Qwen3-4B / Qwen3-8B\"\n        }), \": Models for iPhone and iPad, with Qwen3-8B for high-memory devices\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Qwen3-14B / Qwen3-32B\"\n        }), \": Full-featured models for Mac\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Qwen3-30B-A3B\"\n        }), \": Sparse MoE variant—32B-class performance with only 3B parameters active\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Qwen3-Next-80B / Qwen3-235B-A22B\"\n        }), \": Large models for high-memory Macs (64GB+)\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Qwen3 features hybrid reasoning (toggle between fast and deep thinking), 128K context window, and support for 119 languages.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"performance-on-apple-devices\",\n      children: \"Performance on Apple Devices\"\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"mac-apple-silicon\",\n      children: \"Mac (Apple Silicon)\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"On M1 and newer Macs, Enigmus delivers responsive AI interactions:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"M1/M2 (8GB)\"\n        }), \": Qwen3-0.6B and Qwen3-1.7B run smoothly for everyday tasks\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"M1/M2 Pro (16GB+)\"\n        }), \": GPT-OSS-20b and Qwen3-14B for advanced use cases\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"M3/M4 Max (64GB+)\"\n        }), \": Run large models including Qwen3-32B, Qwen3-Next-80B\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"M3/M4 Max (128GB+)\"\n        }), \": Run the largest models including Qwen3-235B-A22B\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"M5 with Neural Accelerators\"\n        }), \": Optimized matrix operations for fastest inference\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"iphone--ipad-ios-18\",\n      children: \"iPhone \u0026 iPad (iOS 18+)\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Enigmus brings on-device AI to mobile:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"iPhone 13+ / iPad\"\n        }), \": Run Qwen3-0.6B, Qwen3-1.7B, Qwen3-4B, or Qwen3-8B (high-memory devices)\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Increased Memory Entitlement\"\n        }), \": Enables larger models on capable devices\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Metal GPU Required\"\n        }), \": Real device needed (simulators not supported)\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"the-privacy-advantage\",\n      children: \"The Privacy Advantage\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Unlike cloud-based AI services, Enigmus processes everything locally. When asking a question, drafting an email, or analyzing a document:\"\n    }), \"\\n\", _jsxs(_components.ol, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Input stays on the device\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"The AI model runs on Apple Silicon\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"The response is generated locally\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Nothing is uploaded to external servers\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This architecture ensures privacy by design—data remains on the device at all times.\"\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.h2, {\n      id: \"frequently-asked-questions\",\n      children: \"Frequently Asked Questions\"\n    }), \"\\n\", _jsxs(\"details\", {\n      children: [_jsx(\"summary\", {\n        children: _jsx(\"strong\", {\n          children: \"What is MLX and why does Enigmus use it?\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"MLX is Apple's open-source machine learning framework, designed specifically for Apple Silicon. Unlike other ML frameworks, MLX uses a \", _jsx(_components.strong, {\n          children: \"unified memory model\"\n        }), \" where arrays live in shared memory—allowing operations to run on CPU, GPU, or Neural Engine without copying data between them. This makes it exceptionally efficient for running large language models on Mac, iPhone, and iPad.\"]\n      }), _jsx(_components.p, {\n        children: \"At WWDC 2025, Apple announced deeper MLX integration into macOS and iOS, signaling it as a core component of their AI strategy.\"\n      }), _jsx(_components.p, {\n        children: _jsxs(_components.em, {\n          children: [\"Sources: \", _jsx(_components.a, {\n            href: \"https://opensource.apple.com/projects/mlx/\",\n            children: \"Apple MLX Open Source\"\n          }), \" · \", _jsx(_components.a, {\n            href: \"https://github.com/ml-explore/mlx\",\n            children: \"GitHub - ml-explore/mlx\"\n          }), \" · \", _jsx(_components.a, {\n            href: \"https://developer.apple.com/videos/play/wwdc2025/298/\",\n            children: \"WWDC 2025 MLX Session\"\n          })]\n        })\n      })]\n    }), \"\\n\", _jsxs(\"details\", {\n      children: [_jsx(\"summary\", {\n        children: _jsx(\"strong\", {\n          children: \"What's the minimum Mac configuration to run Enigmus?\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Enigmus requires \", _jsx(_components.strong, {\n          children: \"any Mac with Apple Silicon\"\n        }), \" (M1 or newer) running \", _jsx(_components.strong, {\n          children: \"macOS 14 Sonoma\"\n        }), \" or later. The experience scales with your hardware:\"]\n      }), _jsxs(_components.ul, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"8GB RAM\"\n          }), \": Run compact models like Qwen3-0.6B or Qwen3-1.7B for everyday tasks\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"16GB RAM\"\n          }), \": Run mid-size models like GPT-OSS-20b or Qwen3-14B\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"32GB+ RAM\"\n          }), \": Run larger models with better context handling\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"64GB+ RAM\"\n          }), \": Run large models like Qwen3-32B, Qwen3-Next-80B\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"128GB+ RAM\"\n          }), \": Run the largest models like Qwen3-235B-A22B\"]\n        }), \"\\n\"]\n      }), _jsx(_components.p, {\n        children: \"The M5 chips with Neural Accelerators provide the fastest inference thanks to dedicated matrix multiplication hardware.\"\n      }), _jsx(_components.p, {\n        children: _jsxs(_components.em, {\n          children: [\"Sources: \", _jsx(_components.a, {\n            href: \"https://ml-explore.github.io/mlx/build/html/index.html\",\n            children: \"MLX Documentation\"\n          }), \" · \", _jsx(_components.a, {\n            href: \"https://machinelearning.apple.com/research/exploring-llms-mlx-m5\",\n            children: \"Apple M5 Neural Accelerators\"\n          })]\n        })\n      })]\n    }), \"\\n\", _jsxs(\"details\", {\n      children: [_jsx(\"summary\", {\n        children: _jsx(\"strong\", {\n          children: \"Can I run Enigmus on iPhone or iPad?\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Yes. Enigmus supports \", _jsx(_components.strong, {\n          children: \"iOS 18+\"\n        }), \" on devices with sufficient hardware:\"]\n      }), _jsxs(_components.ul, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"iPhone 13\"\n          }), \" or newer (A15 chip or later)\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"iPad\"\n          }), \" with A15 chip or later\"]\n        }), \"\\n\"]\n      }), _jsx(_components.p, {\n        children: \"A real device is required—iOS Simulators don't support the Metal GPU features MLX requires. For larger models, the \\\"Increased Memory Limit\\\" entitlement must be enabled in device settings.\"\n      }), _jsx(_components.p, {\n        children: \"The Qwen3-0.6B and Qwen3-1.7B models run well on all supported devices, with Qwen3-4B and Qwen3-8B available on high-memory devices.\"\n      }), _jsx(_components.p, {\n        children: _jsxs(_components.em, {\n          children: [\"Sources: \", _jsx(_components.a, {\n            href: \"https://medium.com/@cetinibrahim/mlx-swift-run-llms-in-ios-apps-8f89c1123588\",\n            children: \"MLX Swift on iOS\"\n          }), \" · \", _jsx(_components.a, {\n            href: \"https://github.com/ml-explore/mlx-swift\",\n            children: \"GitHub - ml-explore/mlx-swift\"\n          })]\n        })\n      })]\n    }), \"\\n\", _jsxs(\"details\", {\n      children: [_jsx(\"summary\", {\n        children: _jsx(\"strong\", {\n          children: \"What is GPT-OSS and how does it compare to ChatGPT?\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"GPT-OSS is OpenAI's first \", _jsx(_components.strong, {\n          children: \"open-weight model family\"\n        }), \" since GPT-2, released in August 2025. It includes two variants:\"]\n      }), _jsxs(_components.ul, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"gpt-oss-20b\"\n          }), \": 21 billion parameters, fits in 16GB memory\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"gpt-oss-120b\"\n          }), \": 117 billion parameters for high-end systems\"]\n        }), \"\\n\"]\n      }), _jsx(_components.p, {\n        children: \"Both use a mixture-of-experts (MoE) architecture with 4-bit quantization (MXFP4). The gpt-oss-120b matches or exceeds OpenAI's o4-mini on benchmarks for coding, math, and tool use—running entirely on-device with no API costs or data sharing.\"\n      }), _jsx(_components.p, {\n        children: _jsxs(_components.em, {\n          children: [\"Sources: \", _jsx(_components.a, {\n            href: \"https://openai.com/index/introducing-gpt-oss/\",\n            children: \"Introducing GPT-OSS | OpenAI\"\n          }), \" · \", _jsx(_components.a, {\n            href: \"https://openai.com/index/gpt-oss-model-card/\",\n            children: \"GPT-OSS Model Card\"\n          }), \" · \", _jsx(_components.a, {\n            href: \"https://github.com/openai/gpt-oss\",\n            children: \"GitHub - openai/gpt-oss\"\n          })]\n        })\n      })]\n    }), \"\\n\", _jsxs(\"details\", {\n      children: [_jsx(\"summary\", {\n        children: _jsx(\"strong\", {\n          children: \"What makes Qwen3 special for local AI?\"\n        })\n      }), _jsx(_components.p, {\n        children: \"Qwen3, released by Alibaba in April 2025, offers several advantages for local deployment:\"\n      }), _jsxs(_components.ul, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"Hybrid reasoning\"\n          }), \": Toggle between fast responses and deep thinking mode\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"Efficient variants\"\n          }), \": The Qwen3-30B-A3B uses only 3B active parameters while delivering 32B-class performance\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"Massive context\"\n          }), \": 128K token context window (1M tokens in Qwen3-2507)\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"Multilingual\"\n          }), \": Supports 119 languages and dialects\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"Size range\"\n          }), \": From 0.6B (ultra-light) to 32B (full-featured)\"]\n        }), \"\\n\"]\n      }), _jsx(_components.p, {\n        children: \"The compact Qwen3-0.6B and Qwen3-1.7B models work on all supported iOS devices, Qwen3-4B and Qwen3-8B on high-memory devices, while larger variants shine on Mac.\"\n      }), _jsx(_components.p, {\n        children: _jsxs(_components.em, {\n          children: [\"Sources: \", _jsx(_components.a, {\n            href: \"https://techcrunch.com/2025/04/28/alibaba-unveils-qwen-3-a-family-of-hybrid-ai-reasoning-models/\",\n            children: \"Alibaba Qwen3 Announcement\"\n          }), \" · \", _jsx(_components.a, {\n            href: \"https://github.com/QwenLM/Qwen3\",\n            children: \"GitHub - QwenLM/Qwen3\"\n          }), \" · \", _jsx(_components.a, {\n            href: \"https://huggingface.co/Qwen\",\n            children: \"Qwen on Hugging Face\"\n          })]\n        })\n      })]\n    }), \"\\n\", _jsxs(\"details\", {\n      children: [_jsx(\"summary\", {\n        children: _jsx(\"strong\", {\n          children: \"Is my data really private with Enigmus?\"\n        })\n      }), _jsxs(_components.p, {\n        children: [_jsx(_components.strong, {\n          children: \"Yes, completely.\"\n        }), \" Enigmus processes everything on-device using Apple's MLX framework. Here's what that means:\"]\n      }), _jsxs(_components.ol, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"No cloud connection required\"\n          }), \": Once a model is downloaded, Enigmus works entirely offline\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"No data transmission\"\n          }), \": Prompts, documents, and conversations never leave the Mac, iPhone, or iPad\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"No telemetry\"\n          }), \": No usage data, analytics, or interaction information is collected\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"Local ownership\"\n          }), \": Everything stays in local storage under user control\"]\n        }), \"\\n\"]\n      }), _jsx(_components.p, {\n        children: \"This is fundamentally different from cloud AI services like ChatGPT or Claude, which process data on remote servers. With Enigmus, privacy isn't a policy—it's architecture.\"\n      }), _jsx(_components.p, {\n        children: _jsxs(_components.em, {\n          children: [\"Sources: \", _jsx(_components.a, {\n            href: \"https://ml-explore.github.io/mlx/build/html/unified_memory.html\",\n            children: \"MLX Unified Memory Model\"\n          }), \" · \", _jsx(_components.a, {\n            href: \"https://www.swift.org/blog/mlx-swift/\",\n            children: \"On-device ML with MLX Swift\"\n          })]\n        })\n      })]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}}},"__N_SSG":true},"page":"/[regular]","query":{"regular":"technology"},"buildId":"zmtC6CcDt_NMK1l2nUc--","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>