{"pageProps":{"slug":"technology","data":{"frontmatter":{"title":"Technology for Private AI on Apple Devices","image":"/images/ai-technology-review.webp","description":"Enigmus delivers private, on-device AI for Mac, iPhone, and iPad using Apple's MLX framework. By leveraging Apple Silicon's unified memory architecture and Metal GPU acceleration, all processing happens locally—data never leaves the device.","layout":"text2image"},"content":"\n# Private AI, Powered by Apple Silicon\n\nEnigmus is built exclusively for Apple platforms, leveraging **MLX**—Apple's machine learning framework—to deliver private AI on Mac, iPhone, and iPad. By running entirely on-device, data never leaves the hardware.\n\n## Why Apple Silicon?\n\nApple's M-series chips (M1, M2, M3, M4, and M5) revolutionized what's possible for on-device AI. The key innovation is **unified memory architecture**—CPU, GPU, and Neural Engine all share the same memory pool, eliminating the data transfer bottlenecks that plague traditional systems.\n\nThis means large language models can run efficiently without expensive dedicated GPUs. A MacBook, iMac, or iPhone becomes a capable AI workstation.\n\n## MLX: Apple's ML Framework\n\nMLX is Apple's array framework for machine learning, purpose-built for Apple Silicon. At WWDC 2025, Apple signaled MLX as a strategic component of their AI ecosystem, with deep integration into macOS and iOS.\n\n### Key Advantages\n\n- **Unified Memory**: Arrays live in shared memory—operations run on CPU, GPU, or Neural Engine without data copying\n- **Metal GPU Acceleration**: Purpose-built for Apple's Metal framework, maximizing performance on Apple hardware\n- **Native Swift Support**: First-class Swift API makes it perfect for iOS and macOS app development\n- **Neural Engine Integration**: On M5 chips, MLX leverages dedicated Neural Accelerators for matrix operations\n\n### On-Device Benefits\n\nRunning AI locally on Apple devices provides:\n\n- **Complete Privacy**: Conversations and data never leave the device\n- **No API Costs**: No per-token fees or subscription requirements\n- **Offline Capable**: Works without internet once the model is downloaded\n- **Low Latency**: Instant responses without network round-trips\n\n## Supported Models\n\nEnigmus supports leading models optimized for Apple Silicon:\n\n### GPT-OSS by OpenAI\n\nOpenAI's first open-weight models since GPT-2, released August 2025:\n\n- **gpt-oss-20b**: 21B parameters, runs within 16GB memory—ideal for M1/M2/M3 Macs\n- **gpt-oss-120b**: 117B parameters for high-memory configurations\n\nBoth use mixture-of-experts (MoE) architecture with 4-bit quantization, delivering excellent performance on Apple Silicon.\n\n### Qwen3 by Alibaba\n\nAlibaba's hybrid reasoning models, released April 2025:\n\n- **Qwen3-0.6B / Qwen3-1.7B / Qwen3-4B / Qwen3-8B**: Models for iPhone and iPad, with Qwen3-8B for high-memory devices\n- **Qwen3-14B / Qwen3-32B**: Full-featured models for Mac\n- **Qwen3-30B-A3B**: Sparse MoE variant—32B-class performance with only 3B parameters active\n- **Qwen3-Next-80B / Qwen3-235B-A22B**: Large models for high-memory Macs (64GB+)\n\nQwen3 features hybrid reasoning (toggle between fast and deep thinking), 128K context window, and support for 119 languages.\n\n## Performance on Apple Devices\n\n### Mac (Apple Silicon)\n\nOn M1 and newer Macs, Enigmus delivers responsive AI interactions:\n\n- **M1/M2 (8GB)**: Qwen3-0.6B and Qwen3-1.7B run smoothly for everyday tasks\n- **M1/M2 Pro (16GB+)**: GPT-OSS-20b and Qwen3-14B for advanced use cases\n- **M3/M4 Max (64GB+)**: Run large models including Qwen3-32B, Qwen3-Next-80B\n- **M3/M4 Max (128GB+)**: Run the largest models including Qwen3-235B-A22B\n- **M5 with Neural Accelerators**: Optimized matrix operations for fastest inference\n\n### iPhone & iPad (iOS 18+)\n\nEnigmus brings on-device AI to mobile:\n\n- **iPhone 13+ / iPad**: Run Qwen3-0.6B, Qwen3-1.7B, Qwen3-4B, or Qwen3-8B (high-memory devices)\n- **Increased Memory Entitlement**: Enables larger models on capable devices\n- **Metal GPU Required**: Real device needed (simulators not supported)\n\n## The Privacy Advantage\n\nUnlike cloud-based AI services, Enigmus processes everything locally. When asking a question, drafting an email, or analyzing a document:\n\n1. Input stays on the device\n2. The AI model runs on Apple Silicon\n3. The response is generated locally\n4. Nothing is uploaded to external servers\n\nThis architecture ensures privacy by design—data remains on the device at all times.\n\n---\n\n## Frequently Asked Questions\n\n<details>\n<summary><strong>What is MLX and why does Enigmus use it?</strong></summary>\n\nMLX is Apple's open-source machine learning framework, designed specifically for Apple Silicon. Unlike other ML frameworks, MLX uses a **unified memory model** where arrays live in shared memory—allowing operations to run on CPU, GPU, or Neural Engine without copying data between them. This makes it exceptionally efficient for running large language models on Mac, iPhone, and iPad.\n\nAt WWDC 2025, Apple announced deeper MLX integration into macOS and iOS, signaling it as a core component of their AI strategy.\n\n*Sources: [Apple MLX Open Source](https://opensource.apple.com/projects/mlx/) · [GitHub - ml-explore/mlx](https://github.com/ml-explore/mlx) · [WWDC 2025 MLX Session](https://developer.apple.com/videos/play/wwdc2025/298/)*\n\n</details>\n\n<details>\n<summary><strong>What's the minimum Mac configuration to run Enigmus?</strong></summary>\n\nEnigmus requires **any Mac with Apple Silicon** (M1 or newer) running **macOS 14 Sonoma** or later. The experience scales with your hardware:\n\n- **8GB RAM**: Run compact models like Qwen3-0.6B or Qwen3-1.7B for everyday tasks\n- **16GB RAM**: Run mid-size models like GPT-OSS-20b or Qwen3-14B\n- **32GB+ RAM**: Run larger models with better context handling\n- **64GB+ RAM**: Run large models like Qwen3-32B, Qwen3-Next-80B\n- **128GB+ RAM**: Run the largest models like Qwen3-235B-A22B\n\nThe M5 chips with Neural Accelerators provide the fastest inference thanks to dedicated matrix multiplication hardware.\n\n*Sources: [MLX Documentation](https://ml-explore.github.io/mlx/build/html/index.html) · [Apple M5 Neural Accelerators](https://machinelearning.apple.com/research/exploring-llms-mlx-m5)*\n\n</details>\n\n<details>\n<summary><strong>Can I run Enigmus on iPhone or iPad?</strong></summary>\n\nYes. Enigmus supports **iOS 18+** on devices with sufficient hardware:\n\n- **iPhone 13** or newer (A15 chip or later)\n- **iPad** with A15 chip or later\n\nA real device is required—iOS Simulators don't support the Metal GPU features MLX requires. For larger models, the \"Increased Memory Limit\" entitlement must be enabled in device settings.\n\nThe Qwen3-0.6B and Qwen3-1.7B models run well on all supported devices, with Qwen3-4B and Qwen3-8B available on high-memory devices.\n\n*Sources: [MLX Swift on iOS](https://medium.com/@cetinibrahim/mlx-swift-run-llms-in-ios-apps-8f89c1123588) · [GitHub - ml-explore/mlx-swift](https://github.com/ml-explore/mlx-swift)*\n\n</details>\n\n<details>\n<summary><strong>What is GPT-OSS and how does it compare to ChatGPT?</strong></summary>\n\nGPT-OSS is OpenAI's first **open-weight model family** since GPT-2, released in August 2025. It includes two variants:\n\n- **gpt-oss-20b**: 21 billion parameters, fits in 16GB memory\n- **gpt-oss-120b**: 117 billion parameters for high-end systems\n\nBoth use a mixture-of-experts (MoE) architecture with 4-bit quantization (MXFP4). The gpt-oss-120b matches or exceeds OpenAI's o4-mini on benchmarks for coding, math, and tool use—running entirely on-device with no API costs or data sharing.\n\n*Sources: [Introducing GPT-OSS | OpenAI](https://openai.com/index/introducing-gpt-oss/) · [GPT-OSS Model Card](https://openai.com/index/gpt-oss-model-card/) · [GitHub - openai/gpt-oss](https://github.com/openai/gpt-oss)*\n\n</details>\n\n<details>\n<summary><strong>What makes Qwen3 special for local AI?</strong></summary>\n\nQwen3, released by Alibaba in April 2025, offers several advantages for local deployment:\n\n- **Hybrid reasoning**: Toggle between fast responses and deep thinking mode\n- **Efficient variants**: The Qwen3-30B-A3B uses only 3B active parameters while delivering 32B-class performance\n- **Massive context**: 128K token context window (1M tokens in Qwen3-2507)\n- **Multilingual**: Supports 119 languages and dialects\n- **Size range**: From 0.6B (ultra-light) to 32B (full-featured)\n\nThe compact Qwen3-0.6B and Qwen3-1.7B models work on all supported iOS devices, Qwen3-4B and Qwen3-8B on high-memory devices, while larger variants shine on Mac.\n\n*Sources: [Alibaba Qwen3 Announcement](https://techcrunch.com/2025/04/28/alibaba-unveils-qwen-3-a-family-of-hybrid-ai-reasoning-models/) · [GitHub - QwenLM/Qwen3](https://github.com/QwenLM/Qwen3) · [Qwen on Hugging Face](https://huggingface.co/Qwen)*\n\n</details>\n\n<details>\n<summary><strong>Is my data really private with Enigmus?</strong></summary>\n\n**Yes, completely.** Enigmus processes everything on-device using Apple's MLX framework. Here's what that means:\n\n1. **No cloud connection required**: Once a model is downloaded, Enigmus works entirely offline\n2. **No data transmission**: Prompts, documents, and conversations never leave the Mac, iPhone, or iPad\n3. **No telemetry**: No usage data, analytics, or interaction information is collected\n4. **Local ownership**: Everything stays in local storage under user control\n\nThis is fundamentally different from cloud AI services like ChatGPT or Claude, which process data on remote servers. With Enigmus, privacy isn't a policy—it's architecture.\n\n*Sources: [MLX Unified Memory Model](https://ml-explore.github.io/mlx/build/html/unified_memory.html) · [On-device ML with MLX Swift](https://www.swift.org/blog/mlx-swift/)*\n\n</details>\n","mdxContent":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    h1: \"h1\",\n    p: \"p\",\n    strong: \"strong\",\n    h2: \"h2\",\n    h3: \"h3\",\n    ul: \"ul\",\n    li: \"li\",\n    ol: \"ol\",\n    hr: \"hr\",\n    em: \"em\",\n    a: \"a\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.h1, {\n      id: \"private-ai-powered-by-apple-silicon\",\n      children: \"Private AI, Powered by Apple Silicon\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Enigmus is built exclusively for Apple platforms, leveraging \", _jsx(_components.strong, {\n        children: \"MLX\"\n      }), \"—Apple's machine learning framework—to deliver private AI on Mac, iPhone, and iPad. By running entirely on-device, data never leaves the hardware.\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"why-apple-silicon\",\n      children: \"Why Apple Silicon?\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Apple's M-series chips (M1, M2, M3, M4, and M5) revolutionized what's possible for on-device AI. The key innovation is \", _jsx(_components.strong, {\n        children: \"unified memory architecture\"\n      }), \"—CPU, GPU, and Neural Engine all share the same memory pool, eliminating the data transfer bottlenecks that plague traditional systems.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This means large language models can run efficiently without expensive dedicated GPUs. A MacBook, iMac, or iPhone becomes a capable AI workstation.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"mlx-apples-ml-framework\",\n      children: \"MLX: Apple's ML Framework\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"MLX is Apple's array framework for machine learning, purpose-built for Apple Silicon. At WWDC 2025, Apple signaled MLX as a strategic component of their AI ecosystem, with deep integration into macOS and iOS.\"\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"key-advantages\",\n      children: \"Key Advantages\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Unified Memory\"\n        }), \": Arrays live in shared memory—operations run on CPU, GPU, or Neural Engine without data copying\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Metal GPU Acceleration\"\n        }), \": Purpose-built for Apple's Metal framework, maximizing performance on Apple hardware\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Native Swift Support\"\n        }), \": First-class Swift API makes it perfect for iOS and macOS app development\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Neural Engine Integration\"\n        }), \": On M5 chips, MLX leverages dedicated Neural Accelerators for matrix operations\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"on-device-benefits\",\n      children: \"On-Device Benefits\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Running AI locally on Apple devices provides:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Complete Privacy\"\n        }), \": Conversations and data never leave the device\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"No API Costs\"\n        }), \": No per-token fees or subscription requirements\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Offline Capable\"\n        }), \": Works without internet once the model is downloaded\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Low Latency\"\n        }), \": Instant responses without network round-trips\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"supported-models\",\n      children: \"Supported Models\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Enigmus supports leading models optimized for Apple Silicon:\"\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"gpt-oss-by-openai\",\n      children: \"GPT-OSS by OpenAI\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"OpenAI's first open-weight models since GPT-2, released August 2025:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"gpt-oss-20b\"\n        }), \": 21B parameters, runs within 16GB memory—ideal for M1/M2/M3 Macs\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"gpt-oss-120b\"\n        }), \": 117B parameters for high-memory configurations\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Both use mixture-of-experts (MoE) architecture with 4-bit quantization, delivering excellent performance on Apple Silicon.\"\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"qwen3-by-alibaba\",\n      children: \"Qwen3 by Alibaba\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Alibaba's hybrid reasoning models, released April 2025:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Qwen3-0.6B / Qwen3-1.7B / Qwen3-4B / Qwen3-8B\"\n        }), \": Models for iPhone and iPad, with Qwen3-8B for high-memory devices\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Qwen3-14B / Qwen3-32B\"\n        }), \": Full-featured models for Mac\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Qwen3-30B-A3B\"\n        }), \": Sparse MoE variant—32B-class performance with only 3B parameters active\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Qwen3-Next-80B / Qwen3-235B-A22B\"\n        }), \": Large models for high-memory Macs (64GB+)\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Qwen3 features hybrid reasoning (toggle between fast and deep thinking), 128K context window, and support for 119 languages.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"performance-on-apple-devices\",\n      children: \"Performance on Apple Devices\"\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"mac-apple-silicon\",\n      children: \"Mac (Apple Silicon)\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"On M1 and newer Macs, Enigmus delivers responsive AI interactions:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"M1/M2 (8GB)\"\n        }), \": Qwen3-0.6B and Qwen3-1.7B run smoothly for everyday tasks\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"M1/M2 Pro (16GB+)\"\n        }), \": GPT-OSS-20b and Qwen3-14B for advanced use cases\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"M3/M4 Max (64GB+)\"\n        }), \": Run large models including Qwen3-32B, Qwen3-Next-80B\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"M3/M4 Max (128GB+)\"\n        }), \": Run the largest models including Qwen3-235B-A22B\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"M5 with Neural Accelerators\"\n        }), \": Optimized matrix operations for fastest inference\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"iphone--ipad-ios-18\",\n      children: \"iPhone & iPad (iOS 18+)\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Enigmus brings on-device AI to mobile:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"iPhone 13+ / iPad\"\n        }), \": Run Qwen3-0.6B, Qwen3-1.7B, Qwen3-4B, or Qwen3-8B (high-memory devices)\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Increased Memory Entitlement\"\n        }), \": Enables larger models on capable devices\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Metal GPU Required\"\n        }), \": Real device needed (simulators not supported)\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"the-privacy-advantage\",\n      children: \"The Privacy Advantage\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Unlike cloud-based AI services, Enigmus processes everything locally. When asking a question, drafting an email, or analyzing a document:\"\n    }), \"\\n\", _jsxs(_components.ol, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Input stays on the device\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"The AI model runs on Apple Silicon\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"The response is generated locally\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Nothing is uploaded to external servers\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This architecture ensures privacy by design—data remains on the device at all times.\"\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.h2, {\n      id: \"frequently-asked-questions\",\n      children: \"Frequently Asked Questions\"\n    }), \"\\n\", _jsxs(\"details\", {\n      children: [_jsx(\"summary\", {\n        children: _jsx(\"strong\", {\n          children: \"What is MLX and why does Enigmus use it?\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"MLX is Apple's open-source machine learning framework, designed specifically for Apple Silicon. Unlike other ML frameworks, MLX uses a \", _jsx(_components.strong, {\n          children: \"unified memory model\"\n        }), \" where arrays live in shared memory—allowing operations to run on CPU, GPU, or Neural Engine without copying data between them. This makes it exceptionally efficient for running large language models on Mac, iPhone, and iPad.\"]\n      }), _jsx(_components.p, {\n        children: \"At WWDC 2025, Apple announced deeper MLX integration into macOS and iOS, signaling it as a core component of their AI strategy.\"\n      }), _jsx(_components.p, {\n        children: _jsxs(_components.em, {\n          children: [\"Sources: \", _jsx(_components.a, {\n            href: \"https://opensource.apple.com/projects/mlx/\",\n            children: \"Apple MLX Open Source\"\n          }), \" · \", _jsx(_components.a, {\n            href: \"https://github.com/ml-explore/mlx\",\n            children: \"GitHub - ml-explore/mlx\"\n          }), \" · \", _jsx(_components.a, {\n            href: \"https://developer.apple.com/videos/play/wwdc2025/298/\",\n            children: \"WWDC 2025 MLX Session\"\n          })]\n        })\n      })]\n    }), \"\\n\", _jsxs(\"details\", {\n      children: [_jsx(\"summary\", {\n        children: _jsx(\"strong\", {\n          children: \"What's the minimum Mac configuration to run Enigmus?\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Enigmus requires \", _jsx(_components.strong, {\n          children: \"any Mac with Apple Silicon\"\n        }), \" (M1 or newer) running \", _jsx(_components.strong, {\n          children: \"macOS 14 Sonoma\"\n        }), \" or later. The experience scales with your hardware:\"]\n      }), _jsxs(_components.ul, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"8GB RAM\"\n          }), \": Run compact models like Qwen3-0.6B or Qwen3-1.7B for everyday tasks\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"16GB RAM\"\n          }), \": Run mid-size models like GPT-OSS-20b or Qwen3-14B\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"32GB+ RAM\"\n          }), \": Run larger models with better context handling\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"64GB+ RAM\"\n          }), \": Run large models like Qwen3-32B, Qwen3-Next-80B\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"128GB+ RAM\"\n          }), \": Run the largest models like Qwen3-235B-A22B\"]\n        }), \"\\n\"]\n      }), _jsx(_components.p, {\n        children: \"The M5 chips with Neural Accelerators provide the fastest inference thanks to dedicated matrix multiplication hardware.\"\n      }), _jsx(_components.p, {\n        children: _jsxs(_components.em, {\n          children: [\"Sources: \", _jsx(_components.a, {\n            href: \"https://ml-explore.github.io/mlx/build/html/index.html\",\n            children: \"MLX Documentation\"\n          }), \" · \", _jsx(_components.a, {\n            href: \"https://machinelearning.apple.com/research/exploring-llms-mlx-m5\",\n            children: \"Apple M5 Neural Accelerators\"\n          })]\n        })\n      })]\n    }), \"\\n\", _jsxs(\"details\", {\n      children: [_jsx(\"summary\", {\n        children: _jsx(\"strong\", {\n          children: \"Can I run Enigmus on iPhone or iPad?\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"Yes. Enigmus supports \", _jsx(_components.strong, {\n          children: \"iOS 18+\"\n        }), \" on devices with sufficient hardware:\"]\n      }), _jsxs(_components.ul, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"iPhone 13\"\n          }), \" or newer (A15 chip or later)\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"iPad\"\n          }), \" with A15 chip or later\"]\n        }), \"\\n\"]\n      }), _jsx(_components.p, {\n        children: \"A real device is required—iOS Simulators don't support the Metal GPU features MLX requires. For larger models, the \\\"Increased Memory Limit\\\" entitlement must be enabled in device settings.\"\n      }), _jsx(_components.p, {\n        children: \"The Qwen3-0.6B and Qwen3-1.7B models run well on all supported devices, with Qwen3-4B and Qwen3-8B available on high-memory devices.\"\n      }), _jsx(_components.p, {\n        children: _jsxs(_components.em, {\n          children: [\"Sources: \", _jsx(_components.a, {\n            href: \"https://medium.com/@cetinibrahim/mlx-swift-run-llms-in-ios-apps-8f89c1123588\",\n            children: \"MLX Swift on iOS\"\n          }), \" · \", _jsx(_components.a, {\n            href: \"https://github.com/ml-explore/mlx-swift\",\n            children: \"GitHub - ml-explore/mlx-swift\"\n          })]\n        })\n      })]\n    }), \"\\n\", _jsxs(\"details\", {\n      children: [_jsx(\"summary\", {\n        children: _jsx(\"strong\", {\n          children: \"What is GPT-OSS and how does it compare to ChatGPT?\"\n        })\n      }), _jsxs(_components.p, {\n        children: [\"GPT-OSS is OpenAI's first \", _jsx(_components.strong, {\n          children: \"open-weight model family\"\n        }), \" since GPT-2, released in August 2025. It includes two variants:\"]\n      }), _jsxs(_components.ul, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"gpt-oss-20b\"\n          }), \": 21 billion parameters, fits in 16GB memory\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"gpt-oss-120b\"\n          }), \": 117 billion parameters for high-end systems\"]\n        }), \"\\n\"]\n      }), _jsx(_components.p, {\n        children: \"Both use a mixture-of-experts (MoE) architecture with 4-bit quantization (MXFP4). The gpt-oss-120b matches or exceeds OpenAI's o4-mini on benchmarks for coding, math, and tool use—running entirely on-device with no API costs or data sharing.\"\n      }), _jsx(_components.p, {\n        children: _jsxs(_components.em, {\n          children: [\"Sources: \", _jsx(_components.a, {\n            href: \"https://openai.com/index/introducing-gpt-oss/\",\n            children: \"Introducing GPT-OSS | OpenAI\"\n          }), \" · \", _jsx(_components.a, {\n            href: \"https://openai.com/index/gpt-oss-model-card/\",\n            children: \"GPT-OSS Model Card\"\n          }), \" · \", _jsx(_components.a, {\n            href: \"https://github.com/openai/gpt-oss\",\n            children: \"GitHub - openai/gpt-oss\"\n          })]\n        })\n      })]\n    }), \"\\n\", _jsxs(\"details\", {\n      children: [_jsx(\"summary\", {\n        children: _jsx(\"strong\", {\n          children: \"What makes Qwen3 special for local AI?\"\n        })\n      }), _jsx(_components.p, {\n        children: \"Qwen3, released by Alibaba in April 2025, offers several advantages for local deployment:\"\n      }), _jsxs(_components.ul, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"Hybrid reasoning\"\n          }), \": Toggle between fast responses and deep thinking mode\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"Efficient variants\"\n          }), \": The Qwen3-30B-A3B uses only 3B active parameters while delivering 32B-class performance\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"Massive context\"\n          }), \": 128K token context window (1M tokens in Qwen3-2507)\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"Multilingual\"\n          }), \": Supports 119 languages and dialects\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"Size range\"\n          }), \": From 0.6B (ultra-light) to 32B (full-featured)\"]\n        }), \"\\n\"]\n      }), _jsx(_components.p, {\n        children: \"The compact Qwen3-0.6B and Qwen3-1.7B models work on all supported iOS devices, Qwen3-4B and Qwen3-8B on high-memory devices, while larger variants shine on Mac.\"\n      }), _jsx(_components.p, {\n        children: _jsxs(_components.em, {\n          children: [\"Sources: \", _jsx(_components.a, {\n            href: \"https://techcrunch.com/2025/04/28/alibaba-unveils-qwen-3-a-family-of-hybrid-ai-reasoning-models/\",\n            children: \"Alibaba Qwen3 Announcement\"\n          }), \" · \", _jsx(_components.a, {\n            href: \"https://github.com/QwenLM/Qwen3\",\n            children: \"GitHub - QwenLM/Qwen3\"\n          }), \" · \", _jsx(_components.a, {\n            href: \"https://huggingface.co/Qwen\",\n            children: \"Qwen on Hugging Face\"\n          })]\n        })\n      })]\n    }), \"\\n\", _jsxs(\"details\", {\n      children: [_jsx(\"summary\", {\n        children: _jsx(\"strong\", {\n          children: \"Is my data really private with Enigmus?\"\n        })\n      }), _jsxs(_components.p, {\n        children: [_jsx(_components.strong, {\n          children: \"Yes, completely.\"\n        }), \" Enigmus processes everything on-device using Apple's MLX framework. Here's what that means:\"]\n      }), _jsxs(_components.ol, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"No cloud connection required\"\n          }), \": Once a model is downloaded, Enigmus works entirely offline\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"No data transmission\"\n          }), \": Prompts, documents, and conversations never leave the Mac, iPhone, or iPad\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"No telemetry\"\n          }), \": No usage data, analytics, or interaction information is collected\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"Local ownership\"\n          }), \": Everything stays in local storage under user control\"]\n        }), \"\\n\"]\n      }), _jsx(_components.p, {\n        children: \"This is fundamentally different from cloud AI services like ChatGPT or Claude, which process data on remote servers. With Enigmus, privacy isn't a policy—it's architecture.\"\n      }), _jsx(_components.p, {\n        children: _jsxs(_components.em, {\n          children: [\"Sources: \", _jsx(_components.a, {\n            href: \"https://ml-explore.github.io/mlx/build/html/unified_memory.html\",\n            children: \"MLX Unified Memory Model\"\n          }), \" · \", _jsx(_components.a, {\n            href: \"https://www.swift.org/blog/mlx-swift/\",\n            children: \"On-device ML with MLX Swift\"\n          })]\n        })\n      })]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}}},"__N_SSG":true}